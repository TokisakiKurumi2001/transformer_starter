model:
  name: Transformer
  max_positions: 5000
  # Base Transformer setting
  num_blocks:    6
  num_heads:     8
  dim_embed:     512
  dim_pffn:      2048
  drop_prob:     0.1

vocab:
  name: Multi30k
  language_pair: 
  - 'de'
  - 'en'
  src_language: de_core_news_sm
  tgt_language: en_core_web_sm

dataset: 
  name: Multi30k
  language_pair: 
  - 'de'
  - 'en'

optimizer:
  name: torch.optim.Adam
  lr: 0.0001
  betas: 
  - 0.9
  - 0.98
  eps: 1.0e-9

scheduler:
  name: Scheduler
  dim_embed: 512 # make sure this aligns with the model
  warmup_steps: 4000

loss:
  name: TranslationLoss
  label_smoothing: 0.1

epochs: 20
batch_size: 64
